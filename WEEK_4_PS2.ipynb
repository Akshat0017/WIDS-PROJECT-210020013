{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "Y2QRpV39hgFT"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.metrics import f1_score\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount(\"/content/gdrive\")\n",
        "d1 = '/content/gdrive/My Drive/week 4/sentiment analysis_test.csv'\n",
        "d2='/content/gdrive/My Drive/week 4/sentiment analysis_train.csv'\n",
        "\n",
        "\n",
        "\n",
        "import chardet\n",
        "\n",
        "def read_csv(filename):\n",
        "  with open(filename, 'rb') as f:\n",
        "    encoding = chardet.detect(f.read())['encoding']\n",
        "  return pd.read_csv(filename, encoding=encoding)\n",
        "\n",
        "\n",
        "\n",
        "train_data = read_csv(d2)\n",
        "test_data = read_csv(d1)\n",
        "\n",
        "train_size = int(len(train_data) * 0.8)\n",
        "test_size = len(train_data) - train_size\n",
        "train_data, valid_data = train_data[0:train_size], train_data[train_size:len(train_data)]\n",
        "\n",
        "def tokenize(sentence):\n",
        "  tokens = nltk.word_tokenize(sentence)\n",
        "  return tokens\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "train_data['tokens'] = train_data['Sentence'].apply(tokenize)\n",
        "test_data['tokens'] = test_data['Sentence'].apply(tokenize)\n",
        "valid_data['tokens'] = valid_data['Sentence'].apply(tokenize)\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "stop_words = stopwords.words('english')\n",
        "\n",
        "def remove_stopwords(tokens):\n",
        "  filtered_tokens = [token for token in tokens if token not in stop_words]\n",
        "  return filtered_tokens\n",
        "\n",
        "train_data['filtered_tokens'] = train_data['tokens'].apply(remove_stopwords)\n",
        "test_data['filtered_tokens'] = test_data['tokens'].apply(remove_stopwords)\n",
        "valid_data['filtered_tokens'] = valid_data['tokens'].apply(remove_stopwords)\n",
        "\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def lemmatize(tokens):\n",
        "  lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "  return lemmatized_tokens\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "\n",
        "train_data['lemmatized_tokens'] = train_data['filtered_tokens'].apply(lemmatize)\n",
        "test_data['lemmatized_tokens'] = test_data['filtered_tokens'].apply(lemmatize)\n",
        "valid_data['lemmatized_tokens'] = valid_data['filtered_tokens'].apply(lemmatize)\n",
        "\n",
        "from nltk.classify import NaiveBayesClassifier\n",
        "\n",
        "def get_features(tokens):\n",
        "  features = {}\n",
        "  for token in tokens:\n",
        "    features[token] = True\n",
        "  return features\n",
        "\n",
        "train_features = [(get_features(row['lemmatized_tokens']), row['Sentiment']) for index, row in train_data.iterrows()]\n",
        "valid_features = [(get_features(row['lemmatized_tokens']), row['Sentiment']) for index, row in valid_data.iterrows()]\n",
        "test_features = [(get_features(row['lemmatized_tokens']))for index,row in test_data.iterrows()]\n",
        "\n",
        "\n",
        "classifier = NaiveBayesClassifier.train(train_features)\n",
        "predictions = [classifier.classify(row[0]) for row in valid_features]\n",
        "\n",
        "f1 = f1_score(valid_data['Sentiment'], predictions, average='micro')\n",
        "print(\"F1 score:\", f1)\n",
        "# valid_features.row()\n",
        "# test_features\n",
        "predictions2 = [classifier.classify(row) for row in test_features]\n",
        "\n",
        "\n",
        "prediction_df = pd.DataFrame({'Sentence': test_data['Sentence'], 'Prediction': predictions2})\n",
        "prediction_df.to_csv('predictions_ps2.csv', index=False)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RQl57G2FIm4S",
        "outputId": "4da7bce1-b8fd-4cad-bdbc-50222f3707da"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1 score: 0.6628982528263104\n"
          ]
        }
      ]
    }
  ]
}